{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa3a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 31 17:44:47 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:07:00.0  On |                  N/A |\r\n",
      "|  0%   49C    P8    17W / 170W |    761MiB / 12288MiB |      6%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1396      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1917      G   /usr/lib/xorg/Xorg                116MiB |\r\n",
      "|    0   N/A  N/A      2054      G   /usr/bin/gnome-shell               86MiB |\r\n",
      "|    0   N/A  N/A      2469      G   /usr/lib/firefox/firefox          139MiB |\r\n",
      "|    0   N/A  N/A      3827      G   ...odot_v3.4.4-stable_x11.64      371MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump gpu memory\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20095355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ms/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ms/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "INFO:pytorch_pretrained_biggan.model:loading model ../pretrained/biggan-deep-512 from cache at ../pretrained/biggan-deep-512/pytorch_model.bin\n",
      "INFO:pytorch_pretrained_biggan.model:Model config {\n",
      "  \"attention_layer_position\": 8,\n",
      "  \"channel_width\": 128,\n",
      "  \"class_embed_dim\": 128,\n",
      "  \"eps\": 0.0001,\n",
      "  \"layers\": [\n",
      "    [\n",
      "      false,\n",
      "      16,\n",
      "      16\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      16,\n",
      "      16\n",
      "    ],\n",
      "    [\n",
      "      false,\n",
      "      16,\n",
      "      16\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      16,\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      false,\n",
      "      8,\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      8,\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      false,\n",
      "      8,\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      8,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      false,\n",
      "      4,\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      4,\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      false,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      2,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      false,\n",
      "      1,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      true,\n",
      "      1,\n",
      "      1\n",
      "    ]\n",
      "  ],\n",
      "  \"n_stats\": 51,\n",
      "  \"num_classes\": 1000,\n",
      "  \"output_dim\": 512,\n",
      "  \"z_dim\": 128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z_dim = 128\n",
    "n_classes = 1000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# download stuff for one hot encoding of y categories\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# load big gan model\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample, display_in_terminal)\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "model = BigGAN.from_pretrained('../pretrained/biggan-deep-512')#model.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# utils\n",
    "\n",
    "# interpolation funcs\n",
    "def interpolate_linear(v1, v2, num_steps):\n",
    "  vectors = []\n",
    "  for x in np.linspace(0.0, 1.0, num_steps):\n",
    "    vectors.append(v2*x+v1*(1-x))  \n",
    "  return np.array(vectors)\n",
    "\n",
    "def interpolate_hypersphere(v1, v2, num_steps):\n",
    "  v1_norm = np.linalg.norm(v1)\n",
    "  v2_norm = np.linalg.norm(v2)\n",
    "  v2_normalized = v2 * (v1_norm / v2_norm)\n",
    "\n",
    "  vectors = []\n",
    "  for step in range(num_steps):\n",
    "    interpolated = v1 + (v2_normalized - v1) * step / (num_steps - 1)\n",
    "    interpolated_norm =  np.linalg.norm(interpolated)\n",
    "    interpolated_normalized = interpolated * (v1_norm / interpolated_norm)\n",
    "    vectors.append(interpolated_normalized)\n",
    "  return np.array(vectors)\n",
    "\n",
    "# im funcs\n",
    "def convert_to_images(obj):\n",
    "    \"\"\" Convert an output tensor from BigGAN in a list of images.\n",
    "        Params:\n",
    "            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n",
    "        Output:\n",
    "            list of Pillow Images of size (height, width)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install Pillow to use images: pip install Pillow\")\n",
    "\n",
    "    if not isinstance(obj, np.ndarray):\n",
    "        obj = obj.cpu().detach().numpy()\n",
    "\n",
    "    obj = obj.transpose((0, 2, 3, 1))\n",
    "    obj = np.clip(((obj + 1) / 2.0) * 256, 0, 255)\n",
    "\n",
    "    img = []\n",
    "    for i, out in enumerate(obj):\n",
    "        out_array = np.asarray(np.uint8(out), dtype=np.uint8)\n",
    "        img.append(Image.fromarray(out_array))\n",
    "    return img\n",
    "\n",
    "def save_as_images(obj, file_name='output'):\n",
    "    \"\"\" Convert and save an output tensor from BigGAN in a list of saved images.\n",
    "        Params:\n",
    "            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n",
    "            file_name: path and beggingin of filename to save.\n",
    "                Images will be saved as `file_name_{image_number}.png`\n",
    "    \"\"\"\n",
    "    img = convert_to_images(obj)\n",
    "\n",
    "    for i, out in enumerate(img):\n",
    "        current_file_name = file_name + '_%d.png' % i\n",
    "        logger.info(\"Saving image to {}\".format(current_file_name))\n",
    "        out.save(current_file_name, 'png')\n",
    "\n",
    "def save_as_batch_images(obj, file_name='output', batch_i=0):\n",
    "    img = convert_to_images(obj)\n",
    "\n",
    "    for i, out in enumerate(img):\n",
    "        path = os.path.join(file_name, \"{}.png\".format(str(batch_i+i)))\n",
    "        #current_file_name = file_name + '%d.png' % (batch_i + i)\n",
    "        logger.info(\"Saving image to {}\".format(path))\n",
    "        out.save(path, 'png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic image generation\n",
    "\n",
    "# Prepare a input\n",
    "# truncation = 0.4\n",
    "# class_vector = one_hot_from_names(['Blenheim spaniel'], batch_size=5)\n",
    "# noise_vector = truncated_noise_sample(truncation=truncation, batch_size=5, seed=50)\n",
    "\n",
    "# # All in tensors\n",
    "# noise_vector = torch.from_numpy(noise_vector)\n",
    "# class_vector = torch.from_numpy(class_vector)\n",
    "\n",
    "# # If you have a GPU, put everything on cuda\n",
    "# noise_vector = noise_vector.to('cuda')\n",
    "# class_vector = class_vector.to('cuda')\n",
    "# noise_vector.shape\n",
    "\n",
    "# # Generate an image\n",
    "# with torch.no_grad():\n",
    "#     output = model(noise_vector, class_vector, truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize z sample\n",
    "\n",
    "dim_z = 128\n",
    "y = np.linspace(0, dim_z, dim_z)\n",
    "truncation = 1.\n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=50)\n",
    "plt.scatter(y,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d37a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU put back on CPU\n",
    "#output = output.to('cpu')\n",
    "\n",
    "# If you have a sixtel compatible terminal you can display the images in the terminal\n",
    "# (see https://github.com/saitoha/libsixel for details)\n",
    "#display_in_terminal(output)\n",
    "\n",
    "# Save results as png images\n",
    "#save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a112837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare interpolation data \n",
    "num_interps = 20 \n",
    "truncation = 0.3\n",
    "\n",
    "y_v1 = one_hot_from_names(['rhinoceros beetle'], batch_size=1)\n",
    "y_v2 = one_hot_from_names(['roundabout'], batch_size=1)\n",
    "\n",
    "z_v1 = truncated_noise_sample(truncation=truncation, batch_size=1, seed=3213)\n",
    "z_v2 = truncated_noise_sample(truncation=truncation, batch_size=1, seed=1)\n",
    "\n",
    "#create interpolation for both the category and the z vector \n",
    "interps_z = interpolate_hypersphere(z_v1 , z_v2 , num_interps)\n",
    "interps_y = interpolate_hypersphere(y_v1,  y_v2, num_interps)\n",
    "\n",
    "interps_z = interps_z.reshape(num_interps, z_dim)\n",
    "interps_y = interps_y.reshape(num_interps, n_classes)\n",
    "\n",
    "interps_z = torch.from_numpy(interps_z)\n",
    "interps_y = torch.from_numpy(interps_y)\n",
    "\n",
    "interps_z = interps_z.to('cuda')\n",
    "interps_y = interps_y.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated interpolated images\n",
    "with torch.no_grad():\n",
    "    output = model(interps_z, interps_y, truncation)\n",
    "    save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve background\n",
    "\n",
    "#In this expermient we try to change the background of an arbitrary image \n",
    "#while keeping the foreground the same. We want the foregournd image to stay stable \n",
    "#so we need a function that preserves the values near zero but at the same time change the values away from zero.\n",
    "\n",
    "# vals near 0 = background features?\n",
    "# vals away from 0 = foreground features?\n",
    "\n",
    "truncation = 0.7\n",
    "\n",
    "y = one_hot_from_names(['tiger beetle'], batch_size=2)\n",
    "\n",
    "z = np.zeros((2,128))\n",
    "z_tmp = truncated_noise_sample(truncation=truncation, batch_size=1, seed=123)\n",
    "z[0] = z_tmp[0]\n",
    "# use sin func to modify values, values close to 0 stay close to 0, values away from 0 will be changed\n",
    "z[1] = np.sin(z_tmp[0])\n",
    "z = z.astype(np.float32)\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "   output = model(z, y, truncation)\n",
    "   save_as_images(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zooming\n",
    "\n",
    "# We try to zoom into a certain category to see the details of the generated images. \n",
    "# In order to that we need increase the weights of the vectors. \n",
    "# This doesn't work unless each feature in the vector space has either the value 1 or -1. \n",
    "# This is done by dividing each value in the vector by its absolute value namely $z/|z|$. \n",
    "# Then we can provide scaling by multipling by increasing negative values. \n",
    "# Note that this is highly dependent on the seed we are choosing. \n",
    "# Here we choose the seed = 2. \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truncation = .7\n",
    "\n",
    "z_start = truncated_noise_sample(truncation=truncation, batch_size=1, seed=5432)\n",
    "\n",
    "zoom_steps = 5\n",
    "step_size = 0.2\n",
    "\n",
    "y = one_hot_from_names(['tiger beetle'], batch_size=zoom_steps)\n",
    "\n",
    "steps = np.arange(step_size,(zoom_steps*step_size)+step_size,step_size)\n",
    "print(steps)\n",
    "z = [-step*(z_start/np.abs(z_start)) for step in steps]\n",
    "z = np.array(z).reshape(zoom_steps, z_dim)\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebcbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   output = model(z, y, truncation)\n",
    "   save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179916d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breeding\n",
    "\n",
    "# The idea is simple we just average the incoded labels and use the same seed vector. We use the combination \n",
    "# y^=ay1+(1−a)y2\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truncation = 1.\n",
    "a = 0.2\n",
    "\n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=1)\n",
    "y1 = one_hot_from_names(['albatross'], batch_size=1)\n",
    "y2 = one_hot_from_names(['pelican'], batch_size=1)\n",
    "y3 = a*y1+(1-a)*y2\n",
    "\n",
    "z = np.array([z,z,z]).reshape(3, 128)\n",
    "y = np.array([y1,y2,y3]).reshape(3, 1000)\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ccdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   output = model(z, y, truncation)\n",
    "   save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create samples accross N classes from a single latent vector\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "n_images = 500 # N\n",
    "truncation = 0.3\n",
    "\n",
    "# one hot encode y\n",
    "y = np.zeros((n_images, n_classes), dtype=np.float32)\n",
    "for i in range(n_images):\n",
    "    for j in range(n_classes):\n",
    "        y[i, i] = 1.0\n",
    "        \n",
    "# get single latent vector\n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=3213)\n",
    "# clone it N times\n",
    "z = np.tile(z,(n_images,1))\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "   for b in range(int(n_images/batch_size)):\n",
    "       start = b*batch_size\n",
    "       end = start + batch_size\n",
    "       z_ = z[start:end, :]\n",
    "       y_ =  y[start:end, :]\n",
    "       results = model(z_, y_, truncation)\n",
    "       batch_idx = b*batch_size\n",
    "       save_as_batch_images(results, \"./outputs/test2/\", batch_i=batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326eb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create samples from a single latent vector with altering elements\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truncation = 0.3\n",
    "n_samples = 10\n",
    "\n",
    "y = one_hot_from_names(['soap bubble'], batch_size=1)\n",
    "y = np.tile(y,(n_samples,1))\n",
    "        \n",
    "# get single latent vector \n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=3213)\n",
    "# clone it N times\n",
    "z = np.tile(z,(n_samples,1))\n",
    "\n",
    "# set 1 elemet of each latent vec copy to 0\n",
    "lfunc = lambda x: 1.5 if x < 0 else x\n",
    "lfunc = np.vectorize(lfunc)\n",
    "\n",
    "for idx in range(n_samples):\n",
    "    z[idx] = lfunc(z[idx])\n",
    "    \n",
    "print(z.shape)\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')\n",
    "\n",
    "batch_size = 2\n",
    "with torch.no_grad():\n",
    "   for b in range(int(n_samples/batch_size)):\n",
    "       start = b*batch_size\n",
    "       end = start + batch_size\n",
    "       z_ = z[start:end, :]\n",
    "       y_ =  y[start:end, :]\n",
    "       results = model(z_, y_, truncation)\n",
    "       batch_idx = b*batch_size\n",
    "       save_as_batch_images(results, \"./outputs/test3/\", batch_i=batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47204ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "import json\n",
    "\n",
    "def truncated_noise_sample_(batch_size=1, minz=-2, maxz=2, dim_z=128, truncation=1., seed=None):\n",
    "    state = None if seed is None else np.random.RandomState(seed)\n",
    "    values = truncnorm.rvs(minz, maxz, size=(batch_size, dim_z), random_state=state).astype(np.float32)\n",
    "    return truncation * values\n",
    "\n",
    "n_samples = 20\n",
    "minz=-1.\n",
    "maxz= 0.\n",
    "zdim=128\n",
    "truncation=1.0\n",
    "parent_dir = \"./outputs/\"\n",
    "out_dir = \"data_batch\"\n",
    "out_path = os.path.join(parent_dir, out_dir)\n",
    "\n",
    "# create output dirs\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print(\"dir created at '{}' \".format(out_path))\n",
    "\n",
    "im_path = os.path.join(out_path, \"images\")\n",
    "if not os.path.isdir(im_path):\n",
    "    os.mkdir(im_path)\n",
    "    print(\"images dir created at '{}' \".format(im_path))\n",
    "\n",
    "# generate z and y\n",
    "    \n",
    "z = truncated_noise_sample_(n_samples, minz, maxz, zdim, truncation)\n",
    "y = one_hot_from_names(['soap bubble'], batch_size=n_samples)\n",
    "\n",
    "# generate images\n",
    "\n",
    "y_torch = torch.from_numpy(y)\n",
    "z_torch = torch.from_numpy(z)\n",
    "\n",
    "y_torch = y_torch.to('cuda')\n",
    "z_torch = z_torch.to('cuda')\n",
    "\n",
    "torch.cuda.empty_cache() # free up vram\n",
    "predict_batch_size = 5 # generate images in batches to avoid running out of vram\n",
    "with torch.no_grad():\n",
    "    for b in range(int(n_samples/predict_batch_size)):\n",
    "        start = b*predict_batch_size\n",
    "        end = start + predict_batch_size\n",
    "        z_ = z_torch[start:end, :]\n",
    "        y_ =  y_torch[start:end, :]\n",
    "        results = model(z_, y_, truncation)\n",
    "        batch_idx = b*predict_batch_size\n",
    "        save_as_batch_images(results, im_path, batch_i=batch_idx)  \n",
    "\n",
    "# reduce z vecs to 2d\n",
    "import umap\n",
    "n_neighbours = 2 # umap neighbours \n",
    "z_reducer = umap.UMAP(n_neighbors=n_neighbours, min_dist=0.1, n_components=2)\n",
    "z_2d_embeddings = z_reducer.fit_transform(z)\n",
    "\n",
    "\n",
    "# prepare data\n",
    "img_data = []\n",
    "for idx, (emb, vec) in enumerate(zip(z_2d_embeddings, z)):\n",
    "    d = {\"img_idx\": str(idx),\n",
    "         \"z_2d\": emb.tolist(),\n",
    "         \"z\": vec.tolist()}\n",
    "    img_data.append(d)\n",
    "\n",
    "# prepare metadata\n",
    "max_2d_x = z_2d_embeddings[:, 0].max()\n",
    "min_2d_x = z_2d_embeddings[:, 0].min()\n",
    "max_2d_y = z_2d_embeddings[:, 1].max()\n",
    "min_2d_y = z_2d_embeddings[:, 1].min()\n",
    "meta = { \"bounding_box_2d\": \n",
    "            {\"x1\": str(min_2d_x), \n",
    "             \"y1\": str(min_2d_y), \n",
    "             \"x2\": str(max_2d_x), \n",
    "             \"y2\": str(max_2d_y)},\n",
    "         \"img_format\": \"png\"}\n",
    "\n",
    "# export\n",
    "data = {\"meta\" : meta,\n",
    "        \"data\" : img_data}\n",
    "    \n",
    "out_path = os.path.join(out_path, 'data.json')\n",
    "with open(out_path, 'w') as out_file:\n",
    "    json.dump(data, out_file)    \n",
    "\n",
    "\n",
    "# y = torch.from_numpy(y)\n",
    "# z = torch.from_numpy(z)\n",
    "\n",
    "# y = y.to('cuda')\n",
    "# z = z.to('cuda')\n",
    "\n",
    "# predict_batch_size = 10\n",
    "\n",
    "# with torch.no_grad():\n",
    "#    for b in range(int(n_samples/predict_batch_size)):\n",
    "#        start = b*predict_batch_size\n",
    "#        end = start + predict_batch_size\n",
    "#        z_ = z[start:end, :]\n",
    "#        y_ =  y[start:end, :]\n",
    "#        results = model(z_, y_, truncation)\n",
    "#        batch_idx = b*predict_batch_size\n",
    "#        save_as_batch_images(results, path, batch_i=batch_idx)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d04dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/0.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/1.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/2.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/3.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/4.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/5.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/6.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/7.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/8.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/9.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/10.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/11.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/12.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/13.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/14.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/15.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/16.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/17.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/18.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/19.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/20.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/21.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/22.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/23.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/24.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/25.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/26.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/27.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/28.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/soap_bubble/29.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/0.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/1.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/2.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/3.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/4.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/5.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/6.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/7.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/8.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/9.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/10.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/11.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/12.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/13.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/14.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/15.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/16.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/17.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/18.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/19.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/20.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/21.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/22.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/23.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/24.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/25.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/26.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/27.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/28.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/magpie/29.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/0.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/1.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/2.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/3.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/4.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/5.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/6.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/7.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/8.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/9.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/10.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/11.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/12.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/13.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/14.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/15.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/16.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/17.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/18.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/19.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/20.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/21.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/22.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/23.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/24.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/25.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/26.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/27.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/28.png\n",
      "INFO:__main__:Saving image to ./outputs/data_batch1/images/broom/29.png\n"
     ]
    }
   ],
   "source": [
    "# generate multiple batches of images with the same Z and alternating Y\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "import json\n",
    "\n",
    "def truncated_noise_sample_(batch_size=1, minz=-2, maxz=2, dim_z=128, truncation=1., seed=None):\n",
    "    state = None if seed is None else np.random.RandomState(seed)\n",
    "    values = truncnorm.rvs(minz, maxz, size=(batch_size, dim_z), random_state=state).astype(np.float32)\n",
    "    return truncation * values\n",
    "\n",
    "total_classes = 1000\n",
    "n_samples = 30\n",
    "minz=-1.\n",
    "maxz= 0.\n",
    "zdim=128\n",
    "truncation=1.0\n",
    "parent_dir = \"./outputs/\"\n",
    "out_dir = \"data_batch1\"\n",
    "classes = ['soap bubble', 'magpie', 'broom']\n",
    "n_classes = len(classes)\n",
    "\n",
    "# create output dirs\n",
    "\n",
    "# main dir\n",
    "out_path = os.path.join(parent_dir, out_dir)\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print(\"dir created at '{}' \".format(out_path))    \n",
    "\n",
    "# images dir\n",
    "images_path = os.path.join(out_path, \"images\")\n",
    "if not os.path.isdir(images_path):\n",
    "    os.mkdir(images_path)\n",
    "    print(\"im dir created at '{}' \".format(images_path))    \n",
    "\n",
    "# sub dirs for each class\n",
    "images_subpaths = [] # save paths for export\n",
    "for c in classes:\n",
    "    c = c.replace(\" \", \"_\")\n",
    "    im_path = os.path.join(images_path, c)\n",
    "    images_subpaths.append(im_path)\n",
    "    if not os.path.isdir(im_path):\n",
    "        os.mkdir(im_path)\n",
    "        print(\"images dir created at '{}' \".format(im_path))\n",
    "\n",
    "# generate z and y\n",
    "    \n",
    "z = truncated_noise_sample_(n_samples, minz, maxz, zdim, truncation)\n",
    "z = np.tile(z,(n_classes, 1, 1))\n",
    "\n",
    "y_tmp = one_hot_from_names(['soap bubble', 'magpie', 'broom'], batch_size=n_classes)\n",
    "\n",
    "# copy one hot encoded y for each sample to be generated\n",
    "# from (3, 1000) \n",
    "# to (3, 20, 1000)\n",
    "y = np.zeros((n_classes, n_samples, total_classes))\n",
    "for i in range(n_classes):\n",
    "      y[i] = np.tile(y_tmp[i],(n_samples,1))\n",
    "\n",
    "# convert numpy to torch arrays\n",
    "y_torch = torch.from_numpy(y).float()\n",
    "z_torch = torch.from_numpy(z)\n",
    "\n",
    "y_torch = y_torch.to('cuda')\n",
    "z_torch = z_torch.to('cuda')\n",
    "\n",
    "torch.cuda.empty_cache() # free up vram\n",
    "\n",
    "predict_batch_size = 5 # generate images in batches to avoid running out of vram\n",
    "\n",
    "# generate images\n",
    "with torch.no_grad():\n",
    "   for i in range(n_classes):\n",
    "        im_path = images_subpaths[i]\n",
    "        for b in range(int(n_samples/predict_batch_size)):\n",
    "            start = b*predict_batch_size\n",
    "            end = start + predict_batch_size\n",
    "            z_ = z_torch[i, start:end, :]\n",
    "            y_ =  y_torch[i, start:end, :]\n",
    "            results = model(z_, y_, truncation)\n",
    "            batch_idx = b*predict_batch_size\n",
    "            save_as_batch_images(results, im_path, batch_i=batch_idx)  \n",
    "\n",
    "# reduce z vecs to 2d\n",
    "import umap\n",
    "n_neighbours = 5 # umap neighbours \n",
    "z_reducer = umap.UMAP(n_neighbors=n_neighbours, min_dist=0.1, n_components=2)\n",
    "z_2d_embeddings = z_reducer.fit_transform(z[0]) # !!! z[0] WILL ONLY REDUCE THE 1st GENERATION !!!\n",
    "\n",
    "# prepare data\n",
    "img_data = []\n",
    "\n",
    "for c in classes:\n",
    "    c = c.replace(\" \", \"_\")\n",
    "    d = {\"label\" : c}\n",
    "    points = []\n",
    "    for idx, (emb, vec) in enumerate(zip(z_2d_embeddings, z[0])): # !!! z[0] WILL INCLUDE ONLY THE 1st GENERATION !!!\n",
    "        p = {\"img_name\": str(idx), \"z_2d\": emb.tolist()\n",
    "             #\"z\": vec.tolist() # uncomment this to also store Z\n",
    "            }\n",
    "        points.append(p)\n",
    "    d[\"data\"] = points\n",
    "    img_data.append(d)\n",
    "\n",
    "# prepare metadata\n",
    "max_2d_x = z_2d_embeddings[:, 0].max()\n",
    "min_2d_x = z_2d_embeddings[:, 0].min()\n",
    "max_2d_y = z_2d_embeddings[:, 1].max()\n",
    "min_2d_y = z_2d_embeddings[:, 1].min()\n",
    "meta = { \"bounding_box_2d\": \n",
    "            {\"x1\": str(min_2d_x), \n",
    "             \"y1\": str(min_2d_y), \n",
    "             \"x2\": str(max_2d_x), \n",
    "             \"y2\": str(max_2d_y)},\n",
    "         \"img_format\": \"png\"}\n",
    "\n",
    "# export\n",
    "data = {\"meta\" : meta,\n",
    "        \"data\" : img_data}\n",
    "    \n",
    "out_path = os.path.join(out_path, 'data.json')\n",
    "with open(out_path, 'w') as out_file:\n",
    "    json.dump(data, out_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f28a394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0c097e7fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = z_2d_embeddings[:, [0]]\n",
    "y = z_2d_embeddings[:, [1]]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888096b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
