{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump gpu memory\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20095355",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 128\n",
    "n_classes = 1000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# download stuff for one hot encoding of y categories\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# load big gan model\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample, display_in_terminal)\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "model = BigGAN.from_pretrained('../pretrained/biggan-deep-512')#model.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# utils\n",
    "\n",
    "# interpolation funcs\n",
    "def interpolate_linear(v1, v2, num_steps):\n",
    "  vectors = []\n",
    "  for x in np.linspace(0.0, 1.0, num_steps):\n",
    "    vectors.append(v2*x+v1*(1-x))  \n",
    "  return np.array(vectors)\n",
    "\n",
    "def interpolate_hypersphere(v1, v2, num_steps):\n",
    "  v1_norm = np.linalg.norm(v1)\n",
    "  v2_norm = np.linalg.norm(v2)\n",
    "  v2_normalized = v2 * (v1_norm / v2_norm)\n",
    "\n",
    "  vectors = []\n",
    "  for step in range(num_steps):\n",
    "    interpolated = v1 + (v2_normalized - v1) * step / (num_steps - 1)\n",
    "    interpolated_norm =  np.linalg.norm(interpolated)\n",
    "    interpolated_normalized = interpolated * (v1_norm / interpolated_norm)\n",
    "    vectors.append(interpolated_normalized)\n",
    "  return np.array(vectors)\n",
    "\n",
    "# im funcs\n",
    "def convert_to_images(obj):\n",
    "    \"\"\" Convert an output tensor from BigGAN in a list of images.\n",
    "        Params:\n",
    "            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n",
    "        Output:\n",
    "            list of Pillow Images of size (height, width)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install Pillow to use images: pip install Pillow\")\n",
    "\n",
    "    if not isinstance(obj, np.ndarray):\n",
    "        obj = obj.cpu().detach().numpy()\n",
    "\n",
    "    obj = obj.transpose((0, 2, 3, 1))\n",
    "    obj = np.clip(((obj + 1) / 2.0) * 256, 0, 255)\n",
    "\n",
    "    img = []\n",
    "    for i, out in enumerate(obj):\n",
    "        out_array = np.asarray(np.uint8(out), dtype=np.uint8)\n",
    "        img.append(Image.fromarray(out_array))\n",
    "    return img\n",
    "\n",
    "def save_as_images(obj, file_name='output'):\n",
    "    \"\"\" Convert and save an output tensor from BigGAN in a list of saved images.\n",
    "        Params:\n",
    "            obj: tensor or numpy array of shape (batch_size, channels, height, width)\n",
    "            file_name: path and beggingin of filename to save.\n",
    "                Images will be saved as `file_name_{image_number}.png`\n",
    "    \"\"\"\n",
    "    img = convert_to_images(obj)\n",
    "\n",
    "    for i, out in enumerate(img):\n",
    "        current_file_name = file_name + '_%d.png' % i\n",
    "        logger.info(\"Saving image to {}\".format(current_file_name))\n",
    "        out.save(current_file_name, 'png')\n",
    "\n",
    "def save_as_batch_images(obj, file_name='output', batch_i=0):\n",
    "    img = convert_to_images(obj)\n",
    "\n",
    "    for i, out in enumerate(img):\n",
    "        path = os.path.join(file_name, \"{}.png\".format(str(batch_i+i)))\n",
    "        #current_file_name = file_name + '%d.png' % (batch_i + i)\n",
    "        logger.info(\"Saving image to {}\".format(path))\n",
    "        out.save(path, 'png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic image generation\n",
    "\n",
    "# Prepare a input\n",
    "# truncation = 0.4\n",
    "# class_vector = one_hot_from_names(['Blenheim spaniel'], batch_size=5)\n",
    "# noise_vector = truncated_noise_sample(truncation=truncation, batch_size=5, seed=50)\n",
    "\n",
    "# # All in tensors\n",
    "# noise_vector = torch.from_numpy(noise_vector)\n",
    "# class_vector = torch.from_numpy(class_vector)\n",
    "\n",
    "# # If you have a GPU, put everything on cuda\n",
    "# noise_vector = noise_vector.to('cuda')\n",
    "# class_vector = class_vector.to('cuda')\n",
    "# noise_vector.shape\n",
    "\n",
    "# # Generate an image\n",
    "# with torch.no_grad():\n",
    "#     output = model(noise_vector, class_vector, truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize z sample\n",
    "\n",
    "dim_z = 128\n",
    "y = np.linspace(0, dim_z, dim_z)\n",
    "truncation = 1.\n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=50)\n",
    "plt.scatter(y,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d37a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU put back on CPU\n",
    "#output = output.to('cpu')\n",
    "\n",
    "# If you have a sixtel compatible terminal you can display the images in the terminal\n",
    "# (see https://github.com/saitoha/libsixel for details)\n",
    "#display_in_terminal(output)\n",
    "\n",
    "# Save results as png images\n",
    "#save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a112837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare interpolation data \n",
    "num_interps = 20 \n",
    "truncation = 0.3\n",
    "\n",
    "y_v1 = one_hot_from_names(['rhinoceros beetle'], batch_size=1)\n",
    "y_v2 = one_hot_from_names(['roundabout'], batch_size=1)\n",
    "\n",
    "z_v1 = truncated_noise_sample(truncation=truncation, batch_size=1, seed=3213)\n",
    "z_v2 = truncated_noise_sample(truncation=truncation, batch_size=1, seed=1)\n",
    "\n",
    "#create interpolation for both the category and the z vector \n",
    "interps_z = interpolate_hypersphere(z_v1 , z_v2 , num_interps)\n",
    "interps_y = interpolate_hypersphere(y_v1,  y_v2, num_interps)\n",
    "\n",
    "interps_z = interps_z.reshape(num_interps, z_dim)\n",
    "interps_y = interps_y.reshape(num_interps, n_classes)\n",
    "\n",
    "interps_z = torch.from_numpy(interps_z)\n",
    "interps_y = torch.from_numpy(interps_y)\n",
    "\n",
    "interps_z = interps_z.to('cuda')\n",
    "interps_y = interps_y.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated interpolated images\n",
    "with torch.no_grad():\n",
    "    output = model(interps_z, interps_y, truncation)\n",
    "    save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve background\n",
    "\n",
    "#In this expermient we try to change the background of an arbitrary image \n",
    "#while keeping the foreground the same. We want the foregournd image to stay stable \n",
    "#so we need a function that preserves the values near zero but at the same time change the values away from zero.\n",
    "\n",
    "# vals near 0 = background features?\n",
    "# vals away from 0 = foreground features?\n",
    "\n",
    "truncation = 0.7\n",
    "\n",
    "y = one_hot_from_names(['tiger beetle'], batch_size=2)\n",
    "\n",
    "z = np.zeros((2,128))\n",
    "z_tmp = truncated_noise_sample(truncation=truncation, batch_size=1, seed=123)\n",
    "z[0] = z_tmp[0]\n",
    "# use sin func to modify values, values close to 0 stay close to 0, values away from 0 will be changed\n",
    "z[1] = np.sin(z_tmp[0])\n",
    "z = z.astype(np.float32)\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "   output = model(z, y, truncation)\n",
    "   save_as_images(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zooming\n",
    "\n",
    "# We try to zoom into a certain category to see the details of the generated images. \n",
    "# In order to that we need increase the weights of the vectors. \n",
    "# This doesn't work unless each feature in the vector space has either the value 1 or -1. \n",
    "# This is done by dividing each value in the vector by its absolute value namely $z/|z|$. \n",
    "# Then we can provide scaling by multipling by increasing negative values. \n",
    "# Note that this is highly dependent on the seed we are choosing. \n",
    "# Here we choose the seed = 2. \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truncation = .7\n",
    "\n",
    "z_start = truncated_noise_sample(truncation=truncation, batch_size=1, seed=5432)\n",
    "\n",
    "zoom_steps = 5\n",
    "step_size = 0.2\n",
    "\n",
    "y = one_hot_from_names(['tiger beetle'], batch_size=zoom_steps)\n",
    "\n",
    "steps = np.arange(step_size,(zoom_steps*step_size)+step_size,step_size)\n",
    "print(steps)\n",
    "z = [-step*(z_start/np.abs(z_start)) for step in steps]\n",
    "z = np.array(z).reshape(zoom_steps, z_dim)\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebcbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   output = model(z, y, truncation)\n",
    "   save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179916d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breeding\n",
    "\n",
    "# The idea is simple we just average the incoded labels and use the same seed vector. We use the combination \n",
    "# y^=ay1+(1−a)y2\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truncation = 1.\n",
    "a = 0.2\n",
    "\n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=1)\n",
    "y1 = one_hot_from_names(['albatross'], batch_size=1)\n",
    "y2 = one_hot_from_names(['pelican'], batch_size=1)\n",
    "y3 = a*y1+(1-a)*y2\n",
    "\n",
    "z = np.array([z,z,z]).reshape(3, 128)\n",
    "y = np.array([y1,y2,y3]).reshape(3, 1000)\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ccdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "   output = model(z, y, truncation)\n",
    "   save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create samples accross N classes from a single latent vector\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "n_images = 500 # N\n",
    "truncation = 0.3\n",
    "\n",
    "# one hot encode y\n",
    "y = np.zeros((n_images, n_classes), dtype=np.float32)\n",
    "for i in range(n_images):\n",
    "    for j in range(n_classes):\n",
    "        y[i, i] = 1.0\n",
    "        \n",
    "# get single latent vector\n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=3213)\n",
    "# clone it N times\n",
    "z = np.tile(z,(n_images,1))\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "   for b in range(int(n_images/batch_size)):\n",
    "       start = b*batch_size\n",
    "       end = start + batch_size\n",
    "       z_ = z[start:end, :]\n",
    "       y_ =  y[start:end, :]\n",
    "       results = model(z_, y_, truncation)\n",
    "       batch_idx = b*batch_size\n",
    "       save_as_batch_images(results, \"./outputs/test2/\", batch_i=batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326eb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create samples from a single latent vector with altering elements\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truncation = 0.3\n",
    "n_samples = 10\n",
    "\n",
    "y = one_hot_from_names(['soap bubble'], batch_size=1)\n",
    "y = np.tile(y,(n_samples,1))\n",
    "        \n",
    "# get single latent vector \n",
    "z = truncated_noise_sample(truncation=truncation, batch_size=1, seed=3213)\n",
    "# clone it N times\n",
    "z = np.tile(z,(n_samples,1))\n",
    "\n",
    "# set 1 elemet of each latent vec copy to 0\n",
    "lfunc = lambda x: 1.5 if x < 0 else x\n",
    "lfunc = np.vectorize(lfunc)\n",
    "\n",
    "for idx in range(n_samples):\n",
    "    z[idx] = lfunc(z[idx])\n",
    "    \n",
    "print(z.shape)\n",
    "y = torch.from_numpy(y)\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "y = y.to('cuda')\n",
    "z = z.to('cuda')\n",
    "\n",
    "batch_size = 2\n",
    "with torch.no_grad():\n",
    "   for b in range(int(n_samples/batch_size)):\n",
    "       start = b*batch_size\n",
    "       end = start + batch_size\n",
    "       z_ = z[start:end, :]\n",
    "       y_ =  y[start:end, :]\n",
    "       results = model(z_, y_, truncation)\n",
    "       batch_idx = b*batch_size\n",
    "       save_as_batch_images(results, \"./outputs/test3/\", batch_i=batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47204ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "import json\n",
    "\n",
    "def truncated_noise_sample_(batch_size=1, minz=-2, maxz=2, dim_z=128, truncation=1., seed=None):\n",
    "    state = None if seed is None else np.random.RandomState(seed)\n",
    "    values = truncnorm.rvs(minz, maxz, size=(batch_size, dim_z), random_state=state).astype(np.float32)\n",
    "    return truncation * values\n",
    "\n",
    "n_samples = 20\n",
    "minz=-1.\n",
    "maxz= 0.\n",
    "zdim=128\n",
    "truncation=1.0\n",
    "parent_dir = \"./outputs/\"\n",
    "out_dir = \"data_batch\"\n",
    "out_path = os.path.join(parent_dir, out_dir)\n",
    "\n",
    "# create output dirs\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print(\"dir created at '{}' \".format(out_path))\n",
    "\n",
    "im_path = os.path.join(out_path, \"images\")\n",
    "if not os.path.isdir(im_path):\n",
    "    os.mkdir(im_path)\n",
    "    print(\"images dir created at '{}' \".format(im_path))\n",
    "\n",
    "# generate z and y\n",
    "    \n",
    "z = truncated_noise_sample_(n_samples, minz, maxz, zdim, truncation)\n",
    "y = one_hot_from_names(['soap bubble'], batch_size=n_samples)\n",
    "\n",
    "# generate images\n",
    "\n",
    "y_torch = torch.from_numpy(y)\n",
    "z_torch = torch.from_numpy(z)\n",
    "\n",
    "y_torch = y_torch.to('cuda')\n",
    "z_torch = z_torch.to('cuda')\n",
    "\n",
    "torch.cuda.empty_cache() # free up vram\n",
    "predict_batch_size = 5 # generate images in batches to avoid running out of vram\n",
    "with torch.no_grad():\n",
    "    for b in range(int(n_samples/predict_batch_size)):\n",
    "        start = b*predict_batch_size\n",
    "        end = start + predict_batch_size\n",
    "        z_ = z_torch[start:end, :]\n",
    "        y_ =  y_torch[start:end, :]\n",
    "        results = model(z_, y_, truncation)\n",
    "        batch_idx = b*predict_batch_size\n",
    "        save_as_batch_images(results, im_path, batch_i=batch_idx)  \n",
    "\n",
    "# reduce z vecs to 2d\n",
    "import umap\n",
    "n_neighbours = 2 # umap neighbours \n",
    "z_reducer = umap.UMAP(n_neighbors=n_neighbours, min_dist=0.1, n_components=2)\n",
    "z_2d_embeddings = z_reducer.fit_transform(z)\n",
    "\n",
    "\n",
    "# prepare data\n",
    "img_data = []\n",
    "for idx, (emb, vec) in enumerate(zip(z_2d_embeddings, z)):\n",
    "    d = {\"img_idx\": str(idx),\n",
    "         \"z_2d\": emb.tolist(),\n",
    "         \"z\": vec.tolist()}\n",
    "    img_data.append(d)\n",
    "\n",
    "# prepare metadata\n",
    "max_2d_x = z_2d_embeddings[:, 0].max()\n",
    "min_2d_x = z_2d_embeddings[:, 0].min()\n",
    "max_2d_y = z_2d_embeddings[:, 1].max()\n",
    "min_2d_y = z_2d_embeddings[:, 1].min()\n",
    "meta = { \"bounding_box_2d\": \n",
    "            {\"x1\": str(min_2d_x), \n",
    "             \"y1\": str(min_2d_y), \n",
    "             \"x2\": str(max_2d_x), \n",
    "             \"y2\": str(max_2d_y)},\n",
    "         \"img_format\": \"png\"}\n",
    "\n",
    "# export\n",
    "data = {\"meta\" : meta,\n",
    "        \"data\" : img_data}\n",
    "    \n",
    "out_path = os.path.join(out_path, 'data.json')\n",
    "with open(out_path, 'w') as out_file:\n",
    "    json.dump(data, out_file)    \n",
    "\n",
    "\n",
    "# y = torch.from_numpy(y)\n",
    "# z = torch.from_numpy(z)\n",
    "\n",
    "# y = y.to('cuda')\n",
    "# z = z.to('cuda')\n",
    "\n",
    "# predict_batch_size = 10\n",
    "\n",
    "# with torch.no_grad():\n",
    "#    for b in range(int(n_samples/predict_batch_size)):\n",
    "#        start = b*predict_batch_size\n",
    "#        end = start + predict_batch_size\n",
    "#        z_ = z[start:end, :]\n",
    "#        y_ =  y[start:end, :]\n",
    "#        results = model(z_, y_, truncation)\n",
    "#        batch_idx = b*predict_batch_size\n",
    "#        save_as_batch_images(results, path, batch_i=batch_idx)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate multiple batches of images with the same Z and alternating Y\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "import json\n",
    "\n",
    "def truncated_noise_sample_(batch_size=1, minz=-2, maxz=2, dim_z=128, truncation=1., seed=None):\n",
    "    state = None if seed is None else np.random.RandomState(seed)\n",
    "    values = truncnorm.rvs(minz, maxz, size=(batch_size, dim_z), random_state=state).astype(np.float32)\n",
    "    return truncation * values\n",
    "\n",
    "total_classes = 1000\n",
    "n_samples = 20\n",
    "minz=-1.\n",
    "maxz= 0.\n",
    "zdim=128\n",
    "truncation=1.0\n",
    "parent_dir = \"./outputs/\"\n",
    "out_dir = \"data_batch1\"\n",
    "classes = ['soap bubble', 'magpie', 'broom']\n",
    "n_classes = len(classes)\n",
    "\n",
    "# create output dirs\n",
    "\n",
    "# main dir\n",
    "out_path = os.path.join(parent_dir, out_dir)\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print(\"dir created at '{}' \".format(out_path))    \n",
    "\n",
    "# images dir\n",
    "images_path = os.path.join(out_path, \"images\")\n",
    "if not os.path.isdir(images_path):\n",
    "    os.mkdir(images_path)\n",
    "    print(\"im dir created at '{}' \".format(images_path))    \n",
    "\n",
    "# sub dirs for each class\n",
    "images_subpaths = [] # save paths for export\n",
    "for c in classes:\n",
    "    c = c.replace(\" \", \"_\")\n",
    "    im_path = os.path.join(images_path, c)\n",
    "    images_subpaths.append(im_path)\n",
    "    if not os.path.isdir(im_path):\n",
    "        os.mkdir(im_path)\n",
    "        print(\"images dir created at '{}' \".format(im_path))\n",
    "\n",
    "# generate z and y\n",
    "    \n",
    "z = truncated_noise_sample_(n_samples, minz, maxz, zdim, truncation)\n",
    "z = np.tile(z,(n_classes, 1, 1))\n",
    "\n",
    "y_tmp = one_hot_from_names(['soap bubble', 'magpie', 'broom'], batch_size=n_classes)\n",
    "\n",
    "# copy one hot encoded y for each sample to be generated\n",
    "# from (3, 1000) \n",
    "# to (3, 20, 1000)\n",
    "y = np.zeros((n_classes, n_samples, total_classes))\n",
    "for i in range(n_classes):\n",
    "      y[i] = np.tile(y_tmp[i],(n_samples,1))\n",
    "\n",
    "# convert numpy to torch arrays\n",
    "y_torch = torch.from_numpy(y).float()\n",
    "z_torch = torch.from_numpy(z)\n",
    "\n",
    "y_torch = y_torch.to('cuda')\n",
    "z_torch = z_torch.to('cuda')\n",
    "\n",
    "torch.cuda.empty_cache() # free up vram\n",
    "\n",
    "predict_batch_size = 5 # generate images in batches to avoid running out of vram\n",
    "\n",
    "# generate images\n",
    "with torch.no_grad():\n",
    "   for i in range(n_classes):\n",
    "        im_path = images_subpaths[i]\n",
    "        for b in range(int(n_samples/predict_batch_size)):\n",
    "            start = b*predict_batch_size\n",
    "            end = start + predict_batch_size\n",
    "            z_ = z_torch[i, start:end, :]\n",
    "            y_ =  y_torch[i, start:end, :]\n",
    "            results = model(z_, y_, truncation)\n",
    "            batch_idx = b*predict_batch_size\n",
    "            save_as_batch_images(results, im_path, batch_i=batch_idx)  \n",
    "\n",
    "# reduce z vecs to 2d\n",
    "import umap\n",
    "n_neighbours = 2 # umap neighbours \n",
    "z_reducer = umap.UMAP(n_neighbors=n_neighbours, min_dist=0.1, n_components=2)\n",
    "z_2d_embeddings = z_reducer.fit_transform(z[0]) # !!! z[0] WILL ONLY REDUCE THE 1st GENERATION !!!\n",
    "\n",
    "# prepare data\n",
    "img_data = []\n",
    "\n",
    "for c in classes:\n",
    "    d = {\"class_name\" : c}\n",
    "    \n",
    "    for idx, (emb, vec) in enumerate(zip(z_2d_embeddings, z[0])): # !!! z[0] WILL INCLUDE ONLY THE 1st GENERATION !!!\n",
    "        v = {\"img_idx\": str(idx),\n",
    "             \"z_2d\": emb.tolist(),\n",
    "             #\"z\": vec.tolist() # uncomment this to also store Z\n",
    "            }\n",
    "    d[\"data\"] = v\n",
    "\n",
    "    img_data.append(d)\n",
    "\n",
    "# prepare metadata\n",
    "max_2d_x = z_2d_embeddings[:, 0].max()\n",
    "min_2d_x = z_2d_embeddings[:, 0].min()\n",
    "max_2d_y = z_2d_embeddings[:, 1].max()\n",
    "min_2d_y = z_2d_embeddings[:, 1].min()\n",
    "meta = { \"bounding_box_2d\": \n",
    "            {\"x1\": str(min_2d_x), \n",
    "             \"y1\": str(min_2d_y), \n",
    "             \"x2\": str(max_2d_x), \n",
    "             \"y2\": str(max_2d_y)},\n",
    "         \"img_format\": \"png\"}\n",
    "\n",
    "# export\n",
    "data = {\"meta\" : meta,\n",
    "        \"data\" : img_data}\n",
    "    \n",
    "out_path = os.path.join(out_path, 'data.json')\n",
    "with open(out_path, 'w') as out_file:\n",
    "    json.dump(data, out_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7b543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71434f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
